{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773b0fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280/OD315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  \\\n",
       "0     1    14.23        1.71  2.43               15.6        127   \n",
       "1     1    13.20        1.78  2.14               11.2        100   \n",
       "2     1    13.16        2.36  2.67               18.6        101   \n",
       "3     1    14.37        1.95  2.50               16.8        113   \n",
       "4     1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   color_intensity   hue  OD280/OD315_of_diluted_wines  proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#  Import and read csv\n",
    "import pandas as pd \n",
    "wine_df = pd.read_csv(\"resources/winedata.csv\")\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dcdfdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into our features and target arrays \n",
    "features = wine_df.drop(columns='type', axis=1).values\n",
    "pretarget = wine_df['type'].values\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(pretarget)\n",
    "target = encoder.transform(pretarget)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_target = tf.keras.utils.to_categorical(target)\n",
    "\n",
    "#split data into a training and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, dummy_target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6f2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "#scale data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d6f9882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 406\n",
      "Trainable params: 406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the model: \n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "#First Hidden Layer\n",
    "nn.add(tf.keras.layers.Dense(units=13, activation=\"relu\", input_dim=13))\n",
    "\n",
    "#Second Hidden Layer\n",
    "nn.add(tf.keras.layers.Dense(units=13, activation=\"relu\"))\n",
    "\n",
    "#Output Layer\n",
    "nn.add(tf.keras.layers.Dense(units=3, activation=\"sigmoid\"))\n",
    "\n",
    "#Checking the structure\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d5f492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#Compile the Model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ded6484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "133/133 [==============================] - 0s 1ms/sample - loss: 0.7253 - acc: 0.5890\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.6935 - acc: 0.6241\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 0s 37us/sample - loss: 0.6629 - acc: 0.6441\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 0s 38us/sample - loss: 0.6361 - acc: 0.6642\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 0s 73us/sample - loss: 0.6108 - acc: 0.6942\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.5889 - acc: 0.7043\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 0s 65us/sample - loss: 0.5684 - acc: 0.7368\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.5489 - acc: 0.7619\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.5302 - acc: 0.7995\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.5134 - acc: 0.8195\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.4973 - acc: 0.8396\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 0.4822 - acc: 0.8647\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 0s 98us/sample - loss: 0.4673 - acc: 0.8822\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.4529 - acc: 0.8897\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.4386 - acc: 0.8997\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.4247 - acc: 0.9148\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 0s 77us/sample - loss: 0.4108 - acc: 0.9248\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.3973 - acc: 0.9273\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 0s 57us/sample - loss: 0.3832 - acc: 0.9398\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 0s 54us/sample - loss: 0.3700 - acc: 0.9449\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 0s 51us/sample - loss: 0.3563 - acc: 0.9499\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.3433 - acc: 0.9499\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.3304 - acc: 0.9524\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.3180 - acc: 0.9524\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.3057 - acc: 0.9574\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.2937 - acc: 0.9599\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 0s 63us/sample - loss: 0.2819 - acc: 0.9624\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.2706 - acc: 0.9649\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 0s 66us/sample - loss: 0.2592 - acc: 0.9649\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.2485 - acc: 0.9649\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.2383 - acc: 0.9649\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.2287 - acc: 0.9674\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.2193 - acc: 0.9674\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.2104 - acc: 0.9724\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.2020 - acc: 0.9724\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.1936 - acc: 0.9749\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.1857 - acc: 0.9799\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 0s 53us/sample - loss: 0.1783 - acc: 0.9774\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.1710 - acc: 0.9799\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 0s 51us/sample - loss: 0.1642 - acc: 0.9799\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.1577 - acc: 0.9799\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.1517 - acc: 0.9799\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 0s 64us/sample - loss: 0.1459 - acc: 0.9825\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 0s 74us/sample - loss: 0.1401 - acc: 0.9825\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 0s 68us/sample - loss: 0.1346 - acc: 0.9825\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.1295 - acc: 0.9825\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.1243 - acc: 0.9825\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.1194 - acc: 0.9825\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.1149 - acc: 0.9825\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 0s 64us/sample - loss: 0.1105 - acc: 0.9825\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.1063 - acc: 0.9850\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.1022 - acc: 0.9875\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0982 - acc: 0.9875\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 0s 65us/sample - loss: 0.0941 - acc: 0.9875\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0904 - acc: 0.9875\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0870 - acc: 0.9875\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 0s 64us/sample - loss: 0.0835 - acc: 0.9875\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0803 - acc: 0.9875\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0775 - acc: 0.9900\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 0s 41us/sample - loss: 0.0748 - acc: 0.9900\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0722 - acc: 0.9900\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0697 - acc: 0.9900\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0674 - acc: 0.9900\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 0s 59us/sample - loss: 0.0652 - acc: 0.9925\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0632 - acc: 0.9925\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 0s 53us/sample - loss: 0.0612 - acc: 0.9925\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0594 - acc: 0.9925\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 0s 62us/sample - loss: 0.0575 - acc: 0.9925\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0558 - acc: 0.9925\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 0s 59us/sample - loss: 0.0541 - acc: 0.9925\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0524 - acc: 0.9925\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0507 - acc: 0.9925\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0491 - acc: 0.9925\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0476 - acc: 0.9950\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0462 - acc: 0.9950\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0447 - acc: 0.9950\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0435 - acc: 0.9950\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0421 - acc: 0.9950\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0409 - acc: 0.9950\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0398 - acc: 0.9950\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0387 - acc: 0.9950\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 0s 53us/sample - loss: 0.0377 - acc: 0.9950\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 59us/sample - loss: 0.0367 - acc: 0.9950\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0358 - acc: 0.9950\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0349 - acc: 0.9950\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 0s 59us/sample - loss: 0.0341 - acc: 0.9950\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 0s 37us/sample - loss: 0.0331 - acc: 0.9950\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0323 - acc: 0.9950\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 0s 37us/sample - loss: 0.0315 - acc: 0.9950\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 0s 53us/sample - loss: 0.0306 - acc: 0.9950\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 0s 37us/sample - loss: 0.0296 - acc: 0.9975\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 0s 51us/sample - loss: 0.0289 - acc: 0.9975\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0282 - acc: 0.9975\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 0s 51us/sample - loss: 0.0275 - acc: 0.9975\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0269 - acc: 0.9975\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 0s 48us/sample - loss: 0.0263 - acc: 0.9975\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0258 - acc: 0.9975\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 0s 43us/sample - loss: 0.0252 - acc: 0.9975\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0247 - acc: 0.9975\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 0s 37us/sample - loss: 0.0240 - acc: 0.9975\n"
     ]
    }
   ],
   "source": [
    "#Train the Model \n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd2ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03788405838939879, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose='auto')\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f6be3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 406\n",
      "Trainable params: 406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#running a new model with softmax layer added \n",
    "nn2 = tf.keras.models.Sequential()\n",
    "nn2.add(tf.keras.layers.Dense(units=13, activation='relu', input_dim=13))\n",
    "nn2.add(tf.keras.layers.Dense(units=13, activation='relu'))\n",
    "nn2.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n",
    "nn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72ad0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile this model\n",
    "nn2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae2a07ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "133/133 [==============================] - 0s 928us/sample - loss: 0.6797 - acc: 0.5990\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.6534 - acc: 0.6140\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.6299 - acc: 0.6341\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.6092 - acc: 0.6441\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.5896 - acc: 0.6516\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.5717 - acc: 0.6692\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.5545 - acc: 0.6842\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.5359 - acc: 0.7018\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 0s 68us/sample - loss: 0.5190 - acc: 0.7143\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.5022 - acc: 0.7343\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.4845 - acc: 0.7544\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 0s 68us/sample - loss: 0.4675 - acc: 0.7820\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.4497 - acc: 0.8170\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 0s 59us/sample - loss: 0.4315 - acc: 0.8346\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.4134 - acc: 0.8571\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.3950 - acc: 0.8797\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.3757 - acc: 0.8922\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.3570 - acc: 0.9023\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.3375 - acc: 0.9148\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 0s 53us/sample - loss: 0.3189 - acc: 0.9273\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.3002 - acc: 0.9373\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 0s 56us/sample - loss: 0.2829 - acc: 0.9424\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.2655 - acc: 0.9499\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.2497 - acc: 0.9549\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.2342 - acc: 0.9574\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.2190 - acc: 0.9574\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 0s 68us/sample - loss: 0.2055 - acc: 0.9599\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.1926 - acc: 0.9674\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 0s 77us/sample - loss: 0.1797 - acc: 0.9724\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 0s 74us/sample - loss: 0.1680 - acc: 0.9724\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 0s 68us/sample - loss: 0.1577 - acc: 0.9749\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 0s 53us/sample - loss: 0.1476 - acc: 0.9799\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.1387 - acc: 0.9799\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 0s 65us/sample - loss: 0.1305 - acc: 0.9799\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.1230 - acc: 0.9799\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 0s 63us/sample - loss: 0.1163 - acc: 0.9799\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.1099 - acc: 0.9799\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.1041 - acc: 0.9799\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 0s 63us/sample - loss: 0.0988 - acc: 0.9799\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 0s 53us/sample - loss: 0.0940 - acc: 0.9799\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.0895 - acc: 0.9799\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0853 - acc: 0.9799\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0814 - acc: 0.9825\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.0776 - acc: 0.9850\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0741 - acc: 0.9850\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0706 - acc: 0.9875\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0676 - acc: 0.9900\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0645 - acc: 0.9900\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 0s 61us/sample - loss: 0.0617 - acc: 0.9950\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0592 - acc: 0.9950\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0568 - acc: 0.9950\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 0s 58us/sample - loss: 0.0544 - acc: 0.9950\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0523 - acc: 0.9950\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0503 - acc: 0.9950\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0483 - acc: 0.9950\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0464 - acc: 0.9950\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 0s 72us/sample - loss: 0.0445 - acc: 0.9950\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0430 - acc: 0.9950\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0417 - acc: 0.9950\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0402 - acc: 0.9950\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0388 - acc: 0.9975\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0374 - acc: 0.9975\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 0s 51us/sample - loss: 0.0362 - acc: 0.9975\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0346 - acc: 0.9975\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 0s 59us/sample - loss: 0.0331 - acc: 0.9975\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 0s 53us/sample - loss: 0.0318 - acc: 0.9975\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0305 - acc: 0.9975\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 0s 59us/sample - loss: 0.0293 - acc: 0.9975\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0282 - acc: 0.9975\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0271 - acc: 0.9975\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0260 - acc: 0.9975\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 0s 50us/sample - loss: 0.0251 - acc: 0.9975\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0243 - acc: 0.9975\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0234 - acc: 0.9975\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0226 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0217 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0210 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0201 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 0s 50us/sample - loss: 0.0192 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0186 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 0s 53us/sample - loss: 0.0177 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0171 - acc: 1.0000\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 59us/sample - loss: 0.0165 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0155 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 0s 53us/sample - loss: 0.0149 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 0s 38us/sample - loss: 0.0144 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0139 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 0s 37us/sample - loss: 0.0132 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0128 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 0s 37us/sample - loss: 0.0123 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 0s 51us/sample - loss: 0.0119 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 0s 38us/sample - loss: 0.0115 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0111 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 0s 38us/sample - loss: 0.0106 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0103 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 0s 58us/sample - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0093 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Train 2nd model\n",
    "fit_model2 = nn2.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c763b2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.04247101495663325, Accuracy: 0.9851851463317871\n"
     ]
    }
   ],
   "source": [
    "model_loss2, model_accuracy2 = nn2.evaluate(X_test_scaled, y_test, verbose='auto')\n",
    "print(f\"Loss: {model_loss2}, Accuracy: {model_accuracy2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9566a285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280/OD315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6        127           2.80   \n",
       "1    13.20        1.78  2.14               11.2        100           2.65   \n",
       "2    13.16        2.36  2.67               18.6        101           2.80   \n",
       "3    14.37        1.95  2.50               16.8        113           3.85   \n",
       "4    13.24        2.59  2.87               21.0        118           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   OD280/OD315_of_diluted_wines  proline  \n",
       "0                          3.92     1065  \n",
       "1                          3.40     1050  \n",
       "2                          3.17     1185  \n",
       "3                          3.45     1480  \n",
       "4                          2.93      735  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3rd model using PCA results \n",
    "# first I need to rescale the data \n",
    "pca_data = wine_df.copy()\n",
    "pca_data = pca_data.drop(columns=['type'])\n",
    "pca_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8622e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280/OD315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.518613</td>\n",
       "      <td>-0.562250</td>\n",
       "      <td>0.232053</td>\n",
       "      <td>-1.169593</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.034819</td>\n",
       "      <td>-0.659563</td>\n",
       "      <td>1.224884</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>1.847920</td>\n",
       "      <td>1.013009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246290</td>\n",
       "      <td>-0.499413</td>\n",
       "      <td>-0.827996</td>\n",
       "      <td>-2.490847</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.544721</td>\n",
       "      <td>-0.293321</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>1.113449</td>\n",
       "      <td>0.965242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196879</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>1.109334</td>\n",
       "      <td>-0.268738</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.215533</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>2.135968</td>\n",
       "      <td>0.269020</td>\n",
       "      <td>0.318304</td>\n",
       "      <td>0.788587</td>\n",
       "      <td>1.395148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.691550</td>\n",
       "      <td>-0.346811</td>\n",
       "      <td>0.487926</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>0.930918</td>\n",
       "      <td>2.491446</td>\n",
       "      <td>1.466525</td>\n",
       "      <td>-0.981875</td>\n",
       "      <td>1.032155</td>\n",
       "      <td>1.186068</td>\n",
       "      <td>-0.427544</td>\n",
       "      <td>1.184071</td>\n",
       "      <td>2.334574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
       "0  1.518613   -0.562250  0.232053          -1.169593   1.913905   \n",
       "1  0.246290   -0.499413 -0.827996          -2.490847   0.018145   \n",
       "2  0.196879    0.021231  1.109334          -0.268738   0.088358   \n",
       "3  1.691550   -0.346811  0.487926          -0.809251   0.930918   \n",
       "4  0.295700    0.227694  1.840403           0.451946   1.281985   \n",
       "\n",
       "   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "0       0.808997    1.034819             -0.659563         1.224884   \n",
       "1       0.568648    0.733629             -0.820719        -0.544721   \n",
       "2       0.808997    1.215533             -0.498407         2.135968   \n",
       "3       2.491446    1.466525             -0.981875         1.032155   \n",
       "4       0.808997    0.663351              0.226796         0.401404   \n",
       "\n",
       "   color_intensity       hue  OD280/OD315_of_diluted_wines   proline  \n",
       "0         0.251717  0.362177                      1.847920  1.013009  \n",
       "1        -0.293321  0.406051                      1.113449  0.965242  \n",
       "2         0.269020  0.318304                      0.788587  1.395148  \n",
       "3         1.186068 -0.427544                      1.184071  2.334574  \n",
       "4        -0.319276  0.362177                      0.449601 -0.037874  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaling data data\n",
    "scaler = StandardScaler()\n",
    "pca_scaled = scaler.fit_transform(pca_data[[\"alcohol\", \"malic_acid\", \"ash\", \"alcalinity_of_ash\",  \"magnesium\", \"total_phenols\", \"flavanoids\", \"nonflavanoid_phenols\",\"proanthocyanins\", \"color_intensity\", \"hue\", \"OD280/OD315_of_diluted_wines\",\"proline\"]])\n",
    "pca_data_df = pd.DataFrame(pca_scaled, columns=pca_data.columns)\n",
    "pca_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97cf0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "pcaNN = PCA(n_components = .9)\n",
    "pca_scaled = pcaNN.fit_transform(pca_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdcef1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCA1</th>\n",
       "      <th>PCA2</th>\n",
       "      <th>PCA3</th>\n",
       "      <th>PCA4</th>\n",
       "      <th>PCA5</th>\n",
       "      <th>PCA6</th>\n",
       "      <th>PCA7</th>\n",
       "      <th>PCA8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.316751</td>\n",
       "      <td>-1.443463</td>\n",
       "      <td>-0.165739</td>\n",
       "      <td>-0.215631</td>\n",
       "      <td>0.693043</td>\n",
       "      <td>-0.223880</td>\n",
       "      <td>0.596427</td>\n",
       "      <td>0.065139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.209465</td>\n",
       "      <td>0.333393</td>\n",
       "      <td>-2.026457</td>\n",
       "      <td>-0.291358</td>\n",
       "      <td>-0.257655</td>\n",
       "      <td>-0.927120</td>\n",
       "      <td>0.053776</td>\n",
       "      <td>1.024416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.516740</td>\n",
       "      <td>-1.031151</td>\n",
       "      <td>0.982819</td>\n",
       "      <td>0.724902</td>\n",
       "      <td>-0.251033</td>\n",
       "      <td>0.549276</td>\n",
       "      <td>0.424205</td>\n",
       "      <td>-0.344216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.757066</td>\n",
       "      <td>-2.756372</td>\n",
       "      <td>-0.176192</td>\n",
       "      <td>0.567983</td>\n",
       "      <td>-0.311842</td>\n",
       "      <td>0.114431</td>\n",
       "      <td>-0.383337</td>\n",
       "      <td>0.643593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.008908</td>\n",
       "      <td>-0.869831</td>\n",
       "      <td>2.026688</td>\n",
       "      <td>-0.409766</td>\n",
       "      <td>0.298458</td>\n",
       "      <td>-0.406520</td>\n",
       "      <td>0.444074</td>\n",
       "      <td>0.416700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PCA1      PCA2      PCA3      PCA4      PCA5      PCA6      PCA7  \\\n",
       "0  3.316751 -1.443463 -0.165739 -0.215631  0.693043 -0.223880  0.596427   \n",
       "1  2.209465  0.333393 -2.026457 -0.291358 -0.257655 -0.927120  0.053776   \n",
       "2  2.516740 -1.031151  0.982819  0.724902 -0.251033  0.549276  0.424205   \n",
       "3  3.757066 -2.756372 -0.176192  0.567983 -0.311842  0.114431 -0.383337   \n",
       "4  1.008908 -0.869831  2.026688 -0.409766  0.298458 -0.406520  0.444074   \n",
       "\n",
       "       PCA8  \n",
       "0  0.065139  \n",
       "1  1.024416  \n",
       "2 -0.344216  \n",
       "3  0.643593  \n",
       "4  0.416700  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making the pca dataframe\n",
    "pca_df = pd.DataFrame(data=pca_scaled, columns=[\"PCA1\", \"PCA2\", \"PCA3\", \"PCA4\",\"PCA5\", \"PCA6\", \"PCA7\", \"PCA8\"])\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89351e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 13)                117       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 341\n",
      "Trainable params: 341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#run the neural network model \n",
    "nn3 = tf.keras.models.Sequential()\n",
    "\n",
    "#First Hidden Layer\n",
    "nn3.add(tf.keras.layers.Dense(units=13, activation=\"relu\", input_dim=8))\n",
    "\n",
    "#Second Hidden Layer\n",
    "nn3.add(tf.keras.layers.Dense(units=13, activation=\"relu\"))\n",
    "\n",
    "#Output Layer\n",
    "nn3.add(tf.keras.layers.Dense(units=3, activation=\"sigmoid\"))\n",
    "\n",
    "#Checking the structure\n",
    "nn3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a75a8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the Model\n",
    "nn3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8fc25a28",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 178 input samples and 133 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12228\\3648893499.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Train the Model (Use pca data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfit_model3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2686\u001b[0m       \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m         \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_array_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2690\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_array_lengths\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    481\u001b[0m                      \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m                      \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m                      'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    484\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m     raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 178 input samples and 133 target samples."
     ]
    }
   ],
   "source": [
    "#Train the Model (Use pca data)\n",
    "#fit_model3 = nn3.fit(pca_df, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb660e9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (178, 1) was passed for an output of shape (None, 3) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12228\\3711475748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#trying to get around error for now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfit_model3b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwine_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2690\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2691\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m-> 2692\u001b[1;33m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m   2693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m       \u001b[1;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    547\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[0;32m    548\u001b[0m                            \u001b[1;34m' was passed for an output of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m                            \u001b[1;34m' while using as loss `'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m                            \u001b[1;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                            'as the output.')\n",
      "\u001b[1;31mValueError\u001b[0m: A target array with shape (178, 1) was passed for an output of shape (None, 3) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "#trying to get around error for now \n",
    "fit_model3b = nn3.fit(pca_df, wine_df['type'], epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7543e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
