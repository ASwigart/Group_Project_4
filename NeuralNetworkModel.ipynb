{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13caa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280/OD315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  \\\n",
       "0     1    14.23        1.71  2.43               15.6        127   \n",
       "1     1    13.20        1.78  2.14               11.2        100   \n",
       "2     1    13.16        2.36  2.67               18.6        101   \n",
       "3     1    14.37        1.95  2.50               16.8        113   \n",
       "4     1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   color_intensity   hue  OD280/OD315_of_diluted_wines  proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#  Import and read csv\n",
    "import pandas as pd \n",
    "wine_df = pd.read_csv(\"resources/winedata.csv\")\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2060613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into our features and target arrays \n",
    "features = wine_df.drop(columns='type', axis=1)\n",
    "pretarget = wine_df['type']\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(pretarget)\n",
    "target = encoder.transform(pretarget)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_target = tf.keras.utils.to_categorical(target)\n",
    "\n",
    "#split data into a training and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, dummy_target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c504fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "#scale data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45eff294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                224       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 547\n",
      "Trainable params: 547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the model: \n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "#First Hidden Layer\n",
    "nn.add(tf.keras.layers.Dense(units=16, activation=\"relu\", input_dim=13))\n",
    "\n",
    "#Second Hidden Layer\n",
    "nn.add(tf.keras.layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "#Output Layer\n",
    "nn.add(tf.keras.layers.Dense(units=3, activation=\"sigmoid\"))\n",
    "\n",
    "#Checking the structure\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba27eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brevi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#Compile the Model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4997886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "133/133 [==============================] - 0s 818us/sample - loss: 0.7377 - acc: 0.4712\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.7084 - acc: 0.5138\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.6844 - acc: 0.5414\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 0s 98us/sample - loss: 0.6615 - acc: 0.5915\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 0s 95us/sample - loss: 0.6402 - acc: 0.6216\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 0s 101us/sample - loss: 0.6207 - acc: 0.6566\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 0s 101us/sample - loss: 0.6028 - acc: 0.6942\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 0s 120us/sample - loss: 0.5852 - acc: 0.7143\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 0s 136us/sample - loss: 0.5691 - acc: 0.7343\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 0s 98us/sample - loss: 0.5545 - acc: 0.7393\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 0s 103us/sample - loss: 0.5407 - acc: 0.7469\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.5272 - acc: 0.7494\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 0s 121us/sample - loss: 0.5150 - acc: 0.7519\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 0s 128us/sample - loss: 0.5032 - acc: 0.7519\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 0s 106us/sample - loss: 0.4921 - acc: 0.7544\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 0s 135us/sample - loss: 0.4811 - acc: 0.7544\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 0s 123us/sample - loss: 0.4705 - acc: 0.7569\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 0s 121us/sample - loss: 0.4604 - acc: 0.7594\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.4501 - acc: 0.7619\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 0s 125us/sample - loss: 0.4402 - acc: 0.7694\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 0s 126us/sample - loss: 0.4302 - acc: 0.7719\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 0s 122us/sample - loss: 0.4206 - acc: 0.7794\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 0s 114us/sample - loss: 0.4117 - acc: 0.7820\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 0s 122us/sample - loss: 0.4029 - acc: 0.7870\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.3936 - acc: 0.7970\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 0s 157us/sample - loss: 0.3851 - acc: 0.8120\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.3758 - acc: 0.8296\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 0s 142us/sample - loss: 0.3671 - acc: 0.8346\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 0.3581 - acc: 0.8371\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 0s 89us/sample - loss: 0.3496 - acc: 0.8396\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 0s 131us/sample - loss: 0.3411 - acc: 0.8396\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 0s 94us/sample - loss: 0.3328 - acc: 0.8446\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 0s 138us/sample - loss: 0.3246 - acc: 0.8471\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 0s 150us/sample - loss: 0.3159 - acc: 0.8496\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 0s 129us/sample - loss: 0.3072 - acc: 0.8521\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 0s 125us/sample - loss: 0.2986 - acc: 0.8546\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 0s 80us/sample - loss: 0.2904 - acc: 0.8596\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 0.2819 - acc: 0.8647\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.2742 - acc: 0.8647\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 0s 102us/sample - loss: 0.2660 - acc: 0.8722\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 0s 96us/sample - loss: 0.2579 - acc: 0.8722\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.2502 - acc: 0.8772\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.2423 - acc: 0.8872\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.2349 - acc: 0.8897\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.2272 - acc: 0.8997\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 0s 95us/sample - loss: 0.2195 - acc: 0.9048\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 0s 102us/sample - loss: 0.2118 - acc: 0.9123\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 0s 104us/sample - loss: 0.2046 - acc: 0.9248\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 0s 123us/sample - loss: 0.1967 - acc: 0.9348\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 0s 85us/sample - loss: 0.1888 - acc: 0.9449\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 0s 130us/sample - loss: 0.1810 - acc: 0.9574\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 0s 80us/sample - loss: 0.1732 - acc: 0.9624\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 0s 128us/sample - loss: 0.1656 - acc: 0.9674\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.1581 - acc: 0.9699\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 0s 139us/sample - loss: 0.1505 - acc: 0.9749\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.1433 - acc: 0.9799\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 0s 116us/sample - loss: 0.1365 - acc: 0.9825\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 0s 86us/sample - loss: 0.1296 - acc: 0.9875\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 0s 120us/sample - loss: 0.1231 - acc: 0.9875\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.1166 - acc: 0.9875\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0991 - acc: 1.000 - 0s 82us/sample - loss: 0.1103 - acc: 0.9875\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 0.1040 - acc: 0.9875\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 0s 92us/sample - loss: 0.0983 - acc: 0.9900\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0929 - acc: 0.9900\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 0s 86us/sample - loss: 0.0880 - acc: 0.9900\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 0s 59us/sample - loss: 0.0834 - acc: 0.9900\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 0s 98us/sample - loss: 0.0791 - acc: 0.9900\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 0s 99us/sample - loss: 0.0752 - acc: 0.9900\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 0s 102us/sample - loss: 0.0713 - acc: 0.9900\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.0678 - acc: 0.9900\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 0s 92us/sample - loss: 0.0645 - acc: 0.9925\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 0s 94us/sample - loss: 0.0613 - acc: 0.9950\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 0s 86us/sample - loss: 0.0583 - acc: 0.9950\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0557 - acc: 0.9950\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 0s 61us/sample - loss: 0.0531 - acc: 0.9950\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.0508 - acc: 0.9950\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0486 - acc: 0.9950\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 0s 68us/sample - loss: 0.0466 - acc: 0.9950\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0447 - acc: 0.9950\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0429 - acc: 0.9975\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 0s 97us/sample - loss: 0.0412 - acc: 0.9975\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.0397 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.0381 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 0s 102us/sample - loss: 0.0367 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 0s 124us/sample - loss: 0.0355 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0342 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 0s 98us/sample - loss: 0.0331 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 0s 71us/sample - loss: 0.0319 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 0s 114us/sample - loss: 0.0309 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.0299 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 0s 106us/sample - loss: 0.0290 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0280 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0272 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.0264 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 0s 89us/sample - loss: 0.0256 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0248 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0241 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 0s 98us/sample - loss: 0.0235 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 0s 97us/sample - loss: 0.0228 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 0s 104us/sample - loss: 0.0222 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Train the Model \n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b4226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.035126813252766925, Accuracy: 0.9925926327705383\n"
     ]
    }
   ],
   "source": [
    "#scoring the model\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose='auto')\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5297a7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 32)                448       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 1,603\n",
      "Trainable params: 1,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#creating another model with more units (more output layers)\n",
    "nn_un = tf.keras.models.Sequential()\n",
    "nn_un.add(tf.keras.layers.Dense(units=32, activation='relu', input_dim=13))\n",
    "nn_un.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "nn_un.add(tf.keras.layers.Dense(units=3, activation='sigmoid'))\n",
    "nn_un.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b9e018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#following the same process\n",
    "nn_un.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48697462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "133/133 [==============================] - 0s 733us/sample - loss: 0.6755 - acc: 0.5815\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 0s 87us/sample - loss: 0.6322 - acc: 0.6742\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.5949 - acc: 0.7444\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 0s 88us/sample - loss: 0.5605 - acc: 0.8020\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 0s 120us/sample - loss: 0.5284 - acc: 0.8371\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 0s 106us/sample - loss: 0.4973 - acc: 0.8697\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.4682 - acc: 0.8947\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 0s 118us/sample - loss: 0.4395 - acc: 0.9148\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.4118 - acc: 0.9273\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 0s 152us/sample - loss: 0.3845 - acc: 0.9373\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 0s 140us/sample - loss: 0.3579 - acc: 0.9449\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 0s 101us/sample - loss: 0.3329 - acc: 0.9499\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 0s 123us/sample - loss: 0.3088 - acc: 0.9499\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 0s 142us/sample - loss: 0.2863 - acc: 0.9524\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 0s 99us/sample - loss: 0.2643 - acc: 0.9574\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 0s 156us/sample - loss: 0.2446 - acc: 0.9599\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 0s 130us/sample - loss: 0.2252 - acc: 0.9624\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 0s 156us/sample - loss: 0.2072 - acc: 0.9624\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 0s 114us/sample - loss: 0.1903 - acc: 0.9624\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.1753 - acc: 0.9699\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 0s 137us/sample - loss: 0.1616 - acc: 0.9749\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 0s 167us/sample - loss: 0.1493 - acc: 0.9774\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 0s 93us/sample - loss: 0.1383 - acc: 0.9774\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 0s 174us/sample - loss: 0.1283 - acc: 0.9774\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 0s 152us/sample - loss: 0.1191 - acc: 0.9774\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.1111 - acc: 0.9774\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 0s 183us/sample - loss: 0.1036 - acc: 0.9799\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 0s 98us/sample - loss: 0.0966 - acc: 0.9799\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.0904 - acc: 0.9825\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 0s 158us/sample - loss: 0.0846 - acc: 0.9825\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 0s 103us/sample - loss: 0.0793 - acc: 0.9850\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 0s 115us/sample - loss: 0.0744 - acc: 0.9850\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 0s 120us/sample - loss: 0.0702 - acc: 0.9850\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 0s 128us/sample - loss: 0.0663 - acc: 0.9850\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 0s 74us/sample - loss: 0.0626 - acc: 0.9875\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 0s 159us/sample - loss: 0.0593 - acc: 0.9900\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.0564 - acc: 0.9900\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 0s 116us/sample - loss: 0.0536 - acc: 0.9925\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 0s 138us/sample - loss: 0.0509 - acc: 0.9950\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 0s 92us/sample - loss: 0.0485 - acc: 0.9950\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 0s 143us/sample - loss: 0.0462 - acc: 0.9950\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 0s 131us/sample - loss: 0.0441 - acc: 0.9950\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.0421 - acc: 0.9950\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.0403 - acc: 0.9950\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 0s 135us/sample - loss: 0.0385 - acc: 0.9975\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 0s 98us/sample - loss: 0.0368 - acc: 0.9975\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 0s 124us/sample - loss: 0.0354 - acc: 0.9975\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 0s 119us/sample - loss: 0.0339 - acc: 0.9975\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 0s 120us/sample - loss: 0.0325 - acc: 0.9975\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 0s 168us/sample - loss: 0.0312 - acc: 0.9975\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.0299 - acc: 0.9975\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 0s 109us/sample - loss: 0.0286 - acc: 0.9975\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 0s 151us/sample - loss: 0.0273 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 0s 125us/sample - loss: 0.0263 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 0s 177us/sample - loss: 0.0252 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 0s 87us/sample - loss: 0.0243 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 0s 121us/sample - loss: 0.0234 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.0224 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 0s 117us/sample - loss: 0.0215 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 0s 84us/sample - loss: 0.0207 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 0s 74us/sample - loss: 0.0198 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0193 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 0s 79us/sample - loss: 0.0185 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0179 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0171 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0158 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 0s 59us/sample - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0148 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0143 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 0s 78us/sample - loss: 0.0138 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 0s 71us/sample - loss: 0.0133 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 0s 59us/sample - loss: 0.0129 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 0s 77us/sample - loss: 0.0124 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 0s 57us/sample - loss: 0.0121 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 0s 101us/sample - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.0113 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0110 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 0s 74us/sample - loss: 0.0107 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0104 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 0.0101 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 0s 77us/sample - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 0s 88us/sample - loss: 0.0092 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0089 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 0s 107us/sample - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 0s 88us/sample - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 0s 87us/sample - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 0s 65us/sample - loss: 0.0080 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 0s 65us/sample - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 0s 97us/sample - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 0s 59us/sample - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 0s 62us/sample - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 0s 84us/sample - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.0061 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#fitting model\n",
    "fit_model_un = nn_un.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23e16b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.04225423485040665, Accuracy: 0.9777777791023254\n"
     ]
    }
   ],
   "source": [
    "#scoring the model\n",
    "model_loss_un, model_accuracy_un = nn_un.evaluate(X_test_scaled, y_test, verbose='auto')\n",
    "print(f\"Loss: {model_loss_un}, Accuracy: {model_accuracy_un}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06f7fece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 16)                224       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 547\n",
      "Trainable params: 547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#running model 1 with more epochs\n",
    "nn_epoch = tf.keras.models.Sequential()\n",
    "nn_epoch.add(tf.keras.layers.Dense(units=16, activation='relu', input_dim=13))\n",
    "nn_epoch.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
    "nn_epoch.add(tf.keras.layers.Dense(units=3, activation='sigmoid'))\n",
    "nn_epoch.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaa3c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile and fit steps\n",
    "nn_epoch.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882bd31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "133/133 [==============================] - 0s 61us/sample - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 2/200\n",
      "133/133 [==============================] - 0s 62us/sample - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 3/200\n",
      "133/133 [==============================] - 0s 64us/sample - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 4/200\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 5/200\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 6/200\n",
      "133/133 [==============================] - 0s 115us/sample - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 7/200\n",
      "133/133 [==============================] - 0s 95us/sample - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 8/200\n",
      "133/133 [==============================] - 0s 120us/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 9/200\n",
      "133/133 [==============================] - 0s 107us/sample - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 10/200\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 11/200\n",
      "133/133 [==============================] - 0s 132us/sample - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 12/200\n",
      "133/133 [==============================] - 0s 119us/sample - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 13/200\n",
      "133/133 [==============================] - 0s 118us/sample - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 14/200\n",
      "133/133 [==============================] - 0s 183us/sample - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 15/200\n",
      "133/133 [==============================] - 0s 149us/sample - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 16/200\n",
      "133/133 [==============================] - 0s 152us/sample - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 17/200\n",
      "133/133 [==============================] - 0s 132us/sample - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 18/200\n",
      "133/133 [==============================] - 0s 186us/sample - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 19/200\n",
      "133/133 [==============================] - 0s 156us/sample - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 20/200\n",
      "133/133 [==============================] - 0s 125us/sample - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 21/200\n",
      "133/133 [==============================] - 0s 115us/sample - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 22/200\n",
      "133/133 [==============================] - 0s 128us/sample - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 23/200\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 24/200\n",
      "133/133 [==============================] - 0s 130us/sample - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 25/200\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 26/200\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 27/200\n",
      "133/133 [==============================] - 0s 160us/sample - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 28/200\n",
      "133/133 [==============================] - 0s 130us/sample - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 29/200\n",
      "133/133 [==============================] - 0s 114us/sample - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 30/200\n",
      "133/133 [==============================] - 0s 174us/sample - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 31/200\n",
      "133/133 [==============================] - 0s 98us/sample - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 32/200\n",
      "133/133 [==============================] - 0s 120us/sample - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 33/200\n",
      "133/133 [==============================] - 0s 202us/sample - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 34/200\n",
      "133/133 [==============================] - 0s 118us/sample - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 35/200\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 36/200\n",
      "133/133 [==============================] - 0s 169us/sample - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 37/200\n",
      "133/133 [==============================] - 0s 122us/sample - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 38/200\n",
      "133/133 [==============================] - 0s 115us/sample - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 39/200\n",
      "133/133 [==============================] - 0s 126us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 40/200\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 41/200\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 42/200\n",
      "133/133 [==============================] - 0s 96us/sample - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 43/200\n",
      "133/133 [==============================] - 0s 100us/sample - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 44/200\n",
      "133/133 [==============================] - 0s 77us/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 45/200\n",
      "133/133 [==============================] - 0s 143us/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 46/200\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 47/200\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 48/200\n",
      "133/133 [==============================] - 0s 165us/sample - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 49/200\n",
      "133/133 [==============================] - 0s 138us/sample - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 50/200\n",
      "133/133 [==============================] - 0s 114us/sample - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 51/200\n",
      "133/133 [==============================] - 0s 132us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 52/200\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 53/200\n",
      "133/133 [==============================] - 0s 97us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 54/200\n",
      "133/133 [==============================] - 0s 134us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 55/200\n",
      "133/133 [==============================] - 0s 97us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 56/200\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 57/200\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 58/200\n",
      "133/133 [==============================] - 0s 100us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 59/200\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 60/200\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 61/200\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 62/200\n",
      "133/133 [==============================] - 0s 85us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 63/200\n",
      "133/133 [==============================] - 0s 97us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 64/200\n",
      "133/133 [==============================] - 0s 81us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 65/200\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 66/200\n",
      "133/133 [==============================] - 0s 57us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 67/200\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 68/200\n",
      "133/133 [==============================] - 0s 95us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 69/200\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 70/200\n",
      "133/133 [==============================] - 0s 68us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 71/200\n",
      "133/133 [==============================] - 0s 68us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 72/200\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 73/200\n",
      "133/133 [==============================] - 0s 97us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 74/200\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 75/200\n",
      "133/133 [==============================] - 0s 66us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 76/200\n",
      "133/133 [==============================] - 0s 88us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 77/200\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 78/200\n",
      "133/133 [==============================] - 0s 99us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 79/200\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 80/200\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 81/200\n",
      "133/133 [==============================] - 0s 59us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 82/200\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 83/200\n",
      "133/133 [==============================] - 0s 92us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 84/200\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 85/200\n",
      "133/133 [==============================] - 0s 97us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 86/200\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 87/200\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 88/200\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 89/200\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 90/200\n",
      "133/133 [==============================] - 0s 77us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 91/200\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 92/200\n",
      "133/133 [==============================] - 0s 93us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 93/200\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 94/200\n",
      "133/133 [==============================] - 0s 46us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 95/200\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 96/200\n",
      "133/133 [==============================] - 0s 74us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 97/200\n",
      "133/133 [==============================] - 0s 106us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 98/200\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 99/200\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 100/200\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 101/200\n",
      "133/133 [==============================] - 0s 56us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 102/200\n",
      "133/133 [==============================] - 0s 102us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 103/200\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 104/200\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 105/200\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 106/200\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 107/200\n",
      "133/133 [==============================] - 0s 99us/sample - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 108/200\n",
      "133/133 [==============================] - 0s 81us/sample - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 109/200\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 110/200\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 9.8970e-04 - acc: 1.0000\n",
      "Epoch 111/200\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 9.8016e-04 - acc: 1.0000\n",
      "Epoch 112/200\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 9.6891e-04 - acc: 1.0000\n",
      "Epoch 113/200\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 9.5799e-04 - acc: 1.0000\n",
      "Epoch 114/200\n",
      "133/133 [==============================] - 0s 53us/sample - loss: 9.4634e-04 - acc: 1.0000\n",
      "Epoch 115/200\n",
      "133/133 [==============================] - 0s 107us/sample - loss: 9.3265e-04 - acc: 1.0000\n",
      "Epoch 116/200\n",
      "133/133 [==============================] - 0s 94us/sample - loss: 9.2322e-04 - acc: 1.0000\n",
      "Epoch 117/200\n",
      "133/133 [==============================] - 0s 86us/sample - loss: 9.1156e-04 - acc: 1.0000\n",
      "Epoch 118/200\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 9.0176e-04 - acc: 1.0000\n",
      "Epoch 119/200\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 8.9313e-04 - acc: 1.0000\n",
      "Epoch 120/200\n",
      "133/133 [==============================] - 0s 111us/sample - loss: 8.8220e-04 - acc: 1.0000\n",
      "Epoch 121/200\n",
      "133/133 [==============================] - 0s 100us/sample - loss: 8.7200e-04 - acc: 1.0000\n",
      "Epoch 122/200\n",
      "133/133 [==============================] - 0s 94us/sample - loss: 8.6265e-04 - acc: 1.0000\n",
      "Epoch 123/200\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 8.5423e-04 - acc: 1.0000\n",
      "Epoch 124/200\n",
      "133/133 [==============================] - 0s 65us/sample - loss: 8.4571e-04 - acc: 1.0000\n",
      "Epoch 125/200\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 8.3686e-04 - acc: 1.0000\n",
      "Epoch 126/200\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 8.2808e-04 - acc: 1.0000\n",
      "Epoch 127/200\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 8.1882e-04 - acc: 1.0000\n",
      "Epoch 128/200\n",
      "133/133 [==============================] - 0s 89us/sample - loss: 8.0806e-04 - acc: 1.0000\n",
      "Epoch 129/200\n",
      "133/133 [==============================] - 0s 58us/sample - loss: 8.0181e-04 - acc: 1.0000\n",
      "Epoch 130/200\n",
      "133/133 [==============================] - 0s 99us/sample - loss: 7.9407e-04 - acc: 1.0000\n",
      "Epoch 131/200\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 7.8634e-04 - acc: 1.0000\n",
      "Epoch 132/200\n",
      "133/133 [==============================] - 0s 74us/sample - loss: 7.7805e-04 - acc: 1.0000\n",
      "Epoch 133/200\n",
      "133/133 [==============================] - 0s 71us/sample - loss: 7.7067e-04 - acc: 1.0000\n",
      "Epoch 134/200\n",
      "133/133 [==============================] - 0s 76us/sample - loss: 7.6470e-04 - acc: 1.0000\n",
      "Epoch 135/200\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 7.5454e-04 - acc: 1.0000\n",
      "Epoch 136/200\n",
      "133/133 [==============================] - 0s 72us/sample - loss: 7.4520e-04 - acc: 1.0000\n",
      "Epoch 137/200\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 7.3847e-04 - acc: 1.0000\n",
      "Epoch 138/200\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 7.3178e-04 - acc: 1.0000\n",
      "Epoch 139/200\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 7.2325e-04 - acc: 1.0000\n",
      "Epoch 140/200\n",
      "133/133 [==============================] - 0s 106us/sample - loss: 7.1585e-04 - acc: 1.0000\n",
      "Epoch 141/200\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 7.1033e-04 - acc: 1.0000\n",
      "Epoch 142/200\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 7.0277e-04 - acc: 1.0000\n",
      "Epoch 143/200\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 6.9510e-04 - acc: 1.0000\n",
      "Epoch 144/200\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 6.8860e-04 - acc: 1.0000\n",
      "Epoch 145/200\n",
      "133/133 [==============================] - 0s 97us/sample - loss: 6.8194e-04 - acc: 1.0000\n",
      "Epoch 146/200\n",
      "133/133 [==============================] - 0s 74us/sample - loss: 6.7630e-04 - acc: 1.0000\n",
      "Epoch 147/200\n",
      "133/133 [==============================] - 0s 48us/sample - loss: 6.6932e-04 - acc: 1.0000\n",
      "Epoch 148/200\n",
      "133/133 [==============================] - 0s 89us/sample - loss: 6.6246e-04 - acc: 1.0000\n",
      "Epoch 149/200\n",
      "133/133 [==============================] - 0s 79us/sample - loss: 6.5606e-04 - acc: 1.0000\n",
      "Epoch 150/200\n",
      "133/133 [==============================] - 0s 81us/sample - loss: 6.5010e-04 - acc: 1.0000\n",
      "Epoch 151/200\n",
      "133/133 [==============================] - 0s 59us/sample - loss: 6.4301e-04 - acc: 1.0000\n",
      "Epoch 152/200\n",
      "133/133 [==============================] - 0s 93us/sample - loss: 6.3701e-04 - acc: 1.0000\n",
      "Epoch 153/200\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 6.3141e-04 - acc: 1.0000\n",
      "Epoch 154/200\n",
      "133/133 [==============================] - 0s 54us/sample - loss: 6.2533e-04 - acc: 1.0000\n",
      "Epoch 155/200\n",
      "133/133 [==============================] - 0s 63us/sample - loss: 6.1989e-04 - acc: 1.0000\n",
      "Epoch 156/200\n",
      "133/133 [==============================] - 0s 58us/sample - loss: 6.1264e-04 - acc: 1.0000\n",
      "Epoch 157/200\n",
      "133/133 [==============================] - 0s 159us/sample - loss: 6.0857e-04 - acc: 1.0000\n",
      "Epoch 158/200\n",
      "133/133 [==============================] - 0s 108us/sample - loss: 6.0297e-04 - acc: 1.0000\n",
      "Epoch 159/200\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 5.9671e-04 - acc: 1.0000\n",
      "Epoch 160/200\n",
      "133/133 [==============================] - 0s 121us/sample - loss: 5.9153e-04 - acc: 1.0000\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 112us/sample - loss: 5.8636e-04 - acc: 1.0000\n",
      "Epoch 162/200\n",
      "133/133 [==============================] - 0s 100us/sample - loss: 5.8086e-04 - acc: 1.0000\n",
      "Epoch 163/200\n",
      "133/133 [==============================] - 0s 93us/sample - loss: 5.7565e-04 - acc: 1.0000\n",
      "Epoch 164/200\n",
      "133/133 [==============================] - 0s 152us/sample - loss: 5.7028e-04 - acc: 1.0000\n",
      "Epoch 165/200\n",
      "133/133 [==============================] - 0s 133us/sample - loss: 5.6432e-04 - acc: 1.0000\n",
      "Epoch 166/200\n",
      "133/133 [==============================] - 0s 149us/sample - loss: 5.5924e-04 - acc: 1.0000\n",
      "Epoch 167/200\n",
      "133/133 [==============================] - 0s 136us/sample - loss: 5.5553e-04 - acc: 1.0000\n",
      "Epoch 168/200\n",
      "133/133 [==============================] - 0s 145us/sample - loss: 5.4973e-04 - acc: 1.0000\n",
      "Epoch 169/200\n",
      "133/133 [==============================] - 0s 139us/sample - loss: 5.4423e-04 - acc: 1.0000\n",
      "Epoch 170/200\n",
      "133/133 [==============================] - 0s 150us/sample - loss: 5.3976e-04 - acc: 1.0000\n",
      "Epoch 171/200\n",
      "133/133 [==============================] - 0s 147us/sample - loss: 5.3545e-04 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "133/133 [==============================] - 0s 133us/sample - loss: 5.3109e-04 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "133/133 [==============================] - 0s 143us/sample - loss: 5.2659e-04 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "133/133 [==============================] - 0s 129us/sample - loss: 5.2192e-04 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "133/133 [==============================] - 0s 151us/sample - loss: 5.1745e-04 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "133/133 [==============================] - 0s 134us/sample - loss: 5.1286e-04 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "133/133 [==============================] - 0s 139us/sample - loss: 5.0826e-04 - acc: 1.0000\n",
      "Epoch 178/200\n",
      "133/133 [==============================] - 0s 149us/sample - loss: 5.0682e-04 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "133/133 [==============================] - 0s 151us/sample - loss: 5.0089e-04 - acc: 1.0000\n",
      "Epoch 180/200\n",
      "133/133 [==============================] - 0s 150us/sample - loss: 4.9744e-04 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "133/133 [==============================] - 0s 125us/sample - loss: 4.9318e-04 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "133/133 [==============================] - 0s 153us/sample - loss: 4.8892e-04 - acc: 1.0000\n",
      "Epoch 183/200\n",
      "133/133 [==============================] - 0s 142us/sample - loss: 4.8517e-04 - acc: 1.0000\n",
      "Epoch 184/200\n",
      "133/133 [==============================] - 0s 126us/sample - loss: 4.8113e-04 - acc: 1.0000\n",
      "Epoch 185/200\n",
      "133/133 [==============================] - 0s 144us/sample - loss: 4.7602e-04 - acc: 1.0000\n",
      "Epoch 186/200\n",
      "133/133 [==============================] - 0s 144us/sample - loss: 4.7276e-04 - acc: 1.0000\n",
      "Epoch 187/200\n",
      "133/133 [==============================] - 0s 129us/sample - loss: 4.6952e-04 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "133/133 [==============================] - 0s 153us/sample - loss: 4.6544e-04 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 4.6105e-04 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 4.5778e-04 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "133/133 [==============================] - 0s 119us/sample - loss: 4.5362e-04 - acc: 1.0000\n",
      "Epoch 192/200\n",
      "133/133 [==============================] - 0s 94us/sample - loss: 4.5008e-04 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "133/133 [==============================] - 0s 109us/sample - loss: 4.4582e-04 - acc: 1.0000\n",
      "Epoch 194/200\n",
      "133/133 [==============================] - 0s 115us/sample - loss: 4.4240e-04 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "133/133 [==============================] - 0s 151us/sample - loss: 4.3801e-04 - acc: 1.0000\n",
      "Epoch 196/200\n",
      "133/133 [==============================] - 0s 120us/sample - loss: 4.3441e-04 - acc: 1.0000\n",
      "Epoch 197/200\n",
      "133/133 [==============================] - 0s 117us/sample - loss: 4.3148e-04 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "133/133 [==============================] - 0s 170us/sample - loss: 4.2783e-04 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "133/133 [==============================] - 0s 225us/sample - loss: 4.2452e-04 - acc: 1.0000\n",
      "Epoch 200/200\n",
      "133/133 [==============================] - 0s 119us/sample - loss: 4.2123e-04 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#fitting\n",
    "fit_model_epoch = nn_un.fit(X_train_scaled, y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fb3e1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6652809434466892, Accuracy: 0.614814817905426\n"
     ]
    }
   ],
   "source": [
    "#scoring the model\n",
    "model_loss_epoch, model_accuracy_epoch = nn_epoch.evaluate(X_test_scaled, y_test, verbose='auto')\n",
    "print(f\"Loss: {model_loss_epoch}, Accuracy: {model_accuracy_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35cdde8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 16)                224       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 547\n",
      "Trainable params: 547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#running a new model with softmax layer added \n",
    "nn2 = tf.keras.models.Sequential()\n",
    "nn2.add(tf.keras.layers.Dense(units=16, activation='relu', input_dim=13))\n",
    "nn2.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
    "nn2.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n",
    "nn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3f2352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile this model\n",
    "nn2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27cc64b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "133/133 [==============================] - 0s 783us/sample - loss: 0.6440 - acc: 0.6140\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.5954 - acc: 0.6942\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 0s 76us/sample - loss: 0.5552 - acc: 0.7318\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 0s 78us/sample - loss: 0.5188 - acc: 0.7619\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 0s 95us/sample - loss: 0.4898 - acc: 0.7845\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 0s 123us/sample - loss: 0.4618 - acc: 0.8020\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 0s 136us/sample - loss: 0.4359 - acc: 0.8321\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.4124 - acc: 0.8521\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 0.3898 - acc: 0.8647\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.3684 - acc: 0.8697\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 0s 118us/sample - loss: 0.3483 - acc: 0.8822\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 0s 131us/sample - loss: 0.3297 - acc: 0.8922\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 0s 115us/sample - loss: 0.3114 - acc: 0.9048\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.2940 - acc: 0.9098\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 0.2767 - acc: 0.9248\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 0s 116us/sample - loss: 0.2604 - acc: 0.9348\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 0s 117us/sample - loss: 0.2454 - acc: 0.9373\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 0s 106us/sample - loss: 0.2317 - acc: 0.9398\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.2184 - acc: 0.9398\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 0s 89us/sample - loss: 0.2063 - acc: 0.9424\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 0s 115us/sample - loss: 0.1947 - acc: 0.9474\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.1838 - acc: 0.9574\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 0s 108us/sample - loss: 0.1735 - acc: 0.9599\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 0s 134us/sample - loss: 0.1638 - acc: 0.9599\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 0.1549 - acc: 0.9599\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.1463 - acc: 0.9649\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 0s 121us/sample - loss: 0.1387 - acc: 0.9724\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 0s 150us/sample - loss: 0.1315 - acc: 0.9724\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 0s 154us/sample - loss: 0.1246 - acc: 0.9774\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 0s 124us/sample - loss: 0.1181 - acc: 0.9749\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 0s 151us/sample - loss: 0.1114 - acc: 0.9774\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 0s 115us/sample - loss: 0.1052 - acc: 0.9799\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 0s 115us/sample - loss: 0.0999 - acc: 0.9850\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 0s 135us/sample - loss: 0.0950 - acc: 0.9875\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 0s 109us/sample - loss: 0.0906 - acc: 0.9875\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 0s 134us/sample - loss: 0.0864 - acc: 0.9875\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.0827 - acc: 0.9875\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.0791 - acc: 0.9875\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 0s 133us/sample - loss: 0.0758 - acc: 0.9875\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 0s 120us/sample - loss: 0.0725 - acc: 0.9875\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 0s 161us/sample - loss: 0.0696 - acc: 0.9875\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 0s 180us/sample - loss: 0.0666 - acc: 0.9875\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 0s 101us/sample - loss: 0.0640 - acc: 0.9875\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 0s 128us/sample - loss: 0.0614 - acc: 0.9900\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 0s 162us/sample - loss: 0.0589 - acc: 0.9900\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0567 - acc: 0.9900\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 0s 89us/sample - loss: 0.0547 - acc: 0.9900\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.0526 - acc: 0.9900\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 0s 139us/sample - loss: 0.0507 - acc: 0.9900\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 0s 136us/sample - loss: 0.0489 - acc: 0.9900\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 0s 106us/sample - loss: 0.0473 - acc: 0.9900\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 0s 117us/sample - loss: 0.0458 - acc: 0.9900\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 0s 77us/sample - loss: 0.0443 - acc: 0.9900\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 0s 188us/sample - loss: 0.0430 - acc: 0.9900\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 0s 115us/sample - loss: 0.0416 - acc: 0.9900\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 0s 76us/sample - loss: 0.0405 - acc: 0.9900\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.0392 - acc: 0.9900\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 0s 110us/sample - loss: 0.0381 - acc: 0.9900\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 0s 104us/sample - loss: 0.0370 - acc: 0.9900\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0361 - acc: 0.9900\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.0351 - acc: 0.9900\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.0341 - acc: 0.9900\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 0s 78us/sample - loss: 0.0331 - acc: 0.9900\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.0321 - acc: 0.9900\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0313 - acc: 0.9900\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 0s 100us/sample - loss: 0.0304 - acc: 0.9925\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0294 - acc: 0.9950\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 0s 121us/sample - loss: 0.0290 - acc: 0.9950\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 0s 79us/sample - loss: 0.0279 - acc: 0.9950\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 0s 106us/sample - loss: 0.0273 - acc: 0.9950\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0266 - acc: 0.9975\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 0s 110us/sample - loss: 0.0258 - acc: 0.9975\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 0s 107us/sample - loss: 0.0251 - acc: 0.9975\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 0s 136us/sample - loss: 0.0243 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.0237 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 0s 150us/sample - loss: 0.0230 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 0s 122us/sample - loss: 0.0224 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 0s 134us/sample - loss: 0.0218 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 0s 122us/sample - loss: 0.0212 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 0s 106us/sample - loss: 0.0206 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 0s 122us/sample - loss: 0.0202 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 0s 166us/sample - loss: 0.0197 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 0s 119us/sample - loss: 0.0193 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 0s 125us/sample - loss: 0.0190 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 0s 141us/sample - loss: 0.0186 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 0s 159us/sample - loss: 0.0181 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 0s 165us/sample - loss: 0.0178 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 0s 162us/sample - loss: 0.0173 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 0s 158us/sample - loss: 0.0169 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 0s 157us/sample - loss: 0.0163 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 0s 125us/sample - loss: 0.0161 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 0s 200us/sample - loss: 0.0155 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 0s 168us/sample - loss: 0.0151 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 0s 162us/sample - loss: 0.0148 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 0s 173us/sample - loss: 0.0145 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0227 - acc: 1.000 - 0s 176us/sample - loss: 0.0142 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 0s 134us/sample - loss: 0.0140 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 0s 183us/sample - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 0s 149us/sample - loss: 0.0134 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 0s 174us/sample - loss: 0.0132 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Train 2nd model\n",
    "fit_model2 = nn2.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80032d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.027390272749794855, Accuracy: 0.9851851463317871\n"
     ]
    }
   ],
   "source": [
    "#scoring the model\n",
    "model_loss2, model_accuracy2 = nn2.evaluate(X_test_scaled, y_test, verbose='auto')\n",
    "print(f\"Loss: {model_loss2}, Accuracy: {model_accuracy2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90b3b7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280/OD315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.226328</td>\n",
       "      <td>-0.024519</td>\n",
       "      <td>1.101127</td>\n",
       "      <td>-0.241394</td>\n",
       "      <td>0.016101</td>\n",
       "      <td>0.812935</td>\n",
       "      <td>1.216668</td>\n",
       "      <td>-0.502463</td>\n",
       "      <td>2.090070</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.301960</td>\n",
       "      <td>0.770065</td>\n",
       "      <td>1.445831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.079807</td>\n",
       "      <td>-0.280281</td>\n",
       "      <td>-2.368742</td>\n",
       "      <td>-0.559842</td>\n",
       "      <td>-0.251581</td>\n",
       "      <td>-0.073170</td>\n",
       "      <td>0.148537</td>\n",
       "      <td>-0.818807</td>\n",
       "      <td>-0.362996</td>\n",
       "      <td>-0.782781</td>\n",
       "      <td>1.329088</td>\n",
       "      <td>0.481291</td>\n",
       "      <td>-0.107920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.668617</td>\n",
       "      <td>1.866297</td>\n",
       "      <td>1.315759</td>\n",
       "      <td>2.045642</td>\n",
       "      <td>0.083022</td>\n",
       "      <td>-0.119807</td>\n",
       "      <td>0.108607</td>\n",
       "      <td>0.525654</td>\n",
       "      <td>0.176331</td>\n",
       "      <td>-1.338787</td>\n",
       "      <td>-0.168807</td>\n",
       "      <td>0.701309</td>\n",
       "      <td>-1.236434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.475115</td>\n",
       "      <td>-1.001897</td>\n",
       "      <td>-0.937868</td>\n",
       "      <td>0.163904</td>\n",
       "      <td>0.149942</td>\n",
       "      <td>-1.239098</td>\n",
       "      <td>-1.438686</td>\n",
       "      <td>1.316514</td>\n",
       "      <td>-0.362996</td>\n",
       "      <td>1.176913</td>\n",
       "      <td>-1.623905</td>\n",
       "      <td>-1.457623</td>\n",
       "      <td>-0.336894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.036615</td>\n",
       "      <td>-0.673059</td>\n",
       "      <td>0.850725</td>\n",
       "      <td>-0.646691</td>\n",
       "      <td>-0.452343</td>\n",
       "      <td>0.268836</td>\n",
       "      <td>0.967105</td>\n",
       "      <td>-1.135151</td>\n",
       "      <td>1.185393</td>\n",
       "      <td>0.265427</td>\n",
       "      <td>1.200697</td>\n",
       "      <td>1.045088</td>\n",
       "      <td>1.707515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
       "0  0.226328   -0.024519  1.101127          -0.241394   0.016101   \n",
       "1 -1.079807   -0.280281 -2.368742          -0.559842  -0.251581   \n",
       "2 -0.668617    1.866297  1.315759           2.045642   0.083022   \n",
       "3 -0.475115   -1.001897 -0.937868           0.163904   0.149942   \n",
       "4  1.036615   -0.673059  0.850725          -0.646691  -0.452343   \n",
       "\n",
       "   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "0       0.812935    1.216668             -0.502463         2.090070   \n",
       "1      -0.073170    0.148537             -0.818807        -0.362996   \n",
       "2      -0.119807    0.108607              0.525654         0.176331   \n",
       "3      -1.239098   -1.438686              1.316514        -0.362996   \n",
       "4       0.268836    0.967105             -1.135151         1.185393   \n",
       "\n",
       "   color_intensity       hue  OD280/OD315_of_diluted_wines   proline  \n",
       "0         0.301887  0.301960                      0.770065  1.445831  \n",
       "1        -0.782781  1.329088                      0.481291 -0.107920  \n",
       "2        -1.338787 -0.168807                      0.701309 -1.236434  \n",
       "3         1.176913 -1.623905                     -1.457623 -0.336894  \n",
       "4         0.265427  1.200697                      1.045088  1.707515  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SETUP FOR A 3rd MODEL USING PCA\n",
    "#copying x_train_scaled results \n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=[\"alcohol\", \"malic_acid\", \"ash\", \"alcalinity_of_ash\",  \"magnesium\", \"total_phenols\", \"flavanoids\", \"nonflavanoid_phenols\",\"proanthocyanins\", \"color_intensity\", \"hue\", \"OD280/OD315_of_diluted_wines\",\"proline\"])\n",
    "X_train_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46d6950e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280/OD315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.226328</td>\n",
       "      <td>-0.024519</td>\n",
       "      <td>1.101127</td>\n",
       "      <td>-0.241394</td>\n",
       "      <td>0.016101</td>\n",
       "      <td>0.812935</td>\n",
       "      <td>1.216668</td>\n",
       "      <td>-0.502463</td>\n",
       "      <td>2.090070</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.301960</td>\n",
       "      <td>0.770065</td>\n",
       "      <td>1.445831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.079807</td>\n",
       "      <td>-0.280281</td>\n",
       "      <td>-2.368742</td>\n",
       "      <td>-0.559842</td>\n",
       "      <td>-0.251581</td>\n",
       "      <td>-0.073170</td>\n",
       "      <td>0.148537</td>\n",
       "      <td>-0.818807</td>\n",
       "      <td>-0.362996</td>\n",
       "      <td>-0.782781</td>\n",
       "      <td>1.329088</td>\n",
       "      <td>0.481291</td>\n",
       "      <td>-0.107920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.668617</td>\n",
       "      <td>1.866297</td>\n",
       "      <td>1.315759</td>\n",
       "      <td>2.045642</td>\n",
       "      <td>0.083022</td>\n",
       "      <td>-0.119807</td>\n",
       "      <td>0.108607</td>\n",
       "      <td>0.525654</td>\n",
       "      <td>0.176331</td>\n",
       "      <td>-1.338787</td>\n",
       "      <td>-0.168807</td>\n",
       "      <td>0.701309</td>\n",
       "      <td>-1.236434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.475115</td>\n",
       "      <td>-1.001897</td>\n",
       "      <td>-0.937868</td>\n",
       "      <td>0.163904</td>\n",
       "      <td>0.149942</td>\n",
       "      <td>-1.239098</td>\n",
       "      <td>-1.438686</td>\n",
       "      <td>1.316514</td>\n",
       "      <td>-0.362996</td>\n",
       "      <td>1.176913</td>\n",
       "      <td>-1.623905</td>\n",
       "      <td>-1.457623</td>\n",
       "      <td>-0.336894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.036615</td>\n",
       "      <td>-0.673059</td>\n",
       "      <td>0.850725</td>\n",
       "      <td>-0.646691</td>\n",
       "      <td>-0.452343</td>\n",
       "      <td>0.268836</td>\n",
       "      <td>0.967105</td>\n",
       "      <td>-1.135151</td>\n",
       "      <td>1.185393</td>\n",
       "      <td>0.265427</td>\n",
       "      <td>1.200697</td>\n",
       "      <td>1.045088</td>\n",
       "      <td>1.707515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
       "0  0.226328   -0.024519  1.101127          -0.241394   0.016101   \n",
       "1 -1.079807   -0.280281 -2.368742          -0.559842  -0.251581   \n",
       "2 -0.668617    1.866297  1.315759           2.045642   0.083022   \n",
       "3 -0.475115   -1.001897 -0.937868           0.163904   0.149942   \n",
       "4  1.036615   -0.673059  0.850725          -0.646691  -0.452343   \n",
       "\n",
       "   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "0       0.812935    1.216668             -0.502463         2.090070   \n",
       "1      -0.073170    0.148537             -0.818807        -0.362996   \n",
       "2      -0.119807    0.108607              0.525654         0.176331   \n",
       "3      -1.239098   -1.438686              1.316514        -0.362996   \n",
       "4       0.268836    0.967105             -1.135151         1.185393   \n",
       "\n",
       "   color_intensity       hue  OD280/OD315_of_diluted_wines   proline  \n",
       "0         0.301887  0.301960                      0.770065  1.445831  \n",
       "1        -0.782781  1.329088                      0.481291 -0.107920  \n",
       "2        -1.338787 -0.168807                      0.701309 -1.236434  \n",
       "3         1.176913 -1.623905                     -1.457623 -0.336894  \n",
       "4         0.265427  1.200697                      1.045088  1.707515  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3rd model using PCA results \n",
    "# rename to a relevant title\n",
    "pca_data = X_train_scaled_df.copy()\n",
    "pca_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8de10985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "pcaNN = PCA(n_components = .9)\n",
    "pca_results = pcaNN.fit_transform(pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41f8f185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCA1</th>\n",
       "      <th>PCA2</th>\n",
       "      <th>PCA3</th>\n",
       "      <th>PCA4</th>\n",
       "      <th>PCA5</th>\n",
       "      <th>PCA6</th>\n",
       "      <th>PCA7</th>\n",
       "      <th>PCA8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.503736</td>\n",
       "      <td>-1.017953</td>\n",
       "      <td>0.990552</td>\n",
       "      <td>0.666305</td>\n",
       "      <td>0.193723</td>\n",
       "      <td>0.545181</td>\n",
       "      <td>-0.405599</td>\n",
       "      <td>0.264818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.787272</td>\n",
       "      <td>2.218381</td>\n",
       "      <td>-1.747150</td>\n",
       "      <td>-0.059183</td>\n",
       "      <td>0.281471</td>\n",
       "      <td>0.027989</td>\n",
       "      <td>0.439015</td>\n",
       "      <td>0.069478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.099291</td>\n",
       "      <td>0.994751</td>\n",
       "      <td>2.981664</td>\n",
       "      <td>0.848872</td>\n",
       "      <td>0.823151</td>\n",
       "      <td>-0.233518</td>\n",
       "      <td>1.147661</td>\n",
       "      <td>0.204277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.679649</td>\n",
       "      <td>-0.521566</td>\n",
       "      <td>-1.255032</td>\n",
       "      <td>-0.880405</td>\n",
       "      <td>0.122443</td>\n",
       "      <td>1.074327</td>\n",
       "      <td>-1.482421</td>\n",
       "      <td>-0.259028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.823780</td>\n",
       "      <td>-0.809420</td>\n",
       "      <td>0.034059</td>\n",
       "      <td>0.106171</td>\n",
       "      <td>-0.872966</td>\n",
       "      <td>-0.387026</td>\n",
       "      <td>-0.054433</td>\n",
       "      <td>0.863159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PCA1      PCA2      PCA3      PCA4      PCA5      PCA6      PCA7  \\\n",
       "0 -2.503736 -1.017953  0.990552  0.666305  0.193723  0.545181 -0.405599   \n",
       "1 -0.787272  2.218381 -1.747150 -0.059183  0.281471  0.027989  0.439015   \n",
       "2  1.099291  0.994751  2.981664  0.848872  0.823151 -0.233518  1.147661   \n",
       "3  2.679649 -0.521566 -1.255032 -0.880405  0.122443  1.074327 -1.482421   \n",
       "4 -2.823780 -0.809420  0.034059  0.106171 -0.872966 -0.387026 -0.054433   \n",
       "\n",
       "       PCA8  \n",
       "0  0.264818  \n",
       "1  0.069478  \n",
       "2  0.204277  \n",
       "3 -0.259028  \n",
       "4  0.863159  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making the pca dataframe\n",
    "pca_df = pd.DataFrame(data=pca_results, columns=[\"PCA1\", \"PCA2\", \"PCA3\", \"PCA4\",\"PCA5\", \"PCA6\", \"PCA7\", \"PCA8\"])\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb91c302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 13)                117       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 341\n",
      "Trainable params: 341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#run the neural network model \n",
    "nn3 = tf.keras.models.Sequential()\n",
    "\n",
    "#First Hidden Layer\n",
    "nn3.add(tf.keras.layers.Dense(units=13, activation=\"relu\", input_dim=8))\n",
    "\n",
    "#Second Hidden Layer\n",
    "nn3.add(tf.keras.layers.Dense(units=13, activation=\"relu\"))\n",
    "\n",
    "#Output Layer\n",
    "nn3.add(tf.keras.layers.Dense(units=3, activation=\"sigmoid\"))\n",
    "\n",
    "#Checking the structure\n",
    "nn3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f085a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the Model\n",
    "nn3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b3851a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "133/133 [==============================] - 0s 856us/sample - loss: 0.7168 - acc: 0.5313\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 0s 68us/sample - loss: 0.7032 - acc: 0.5614\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 0s 73us/sample - loss: 0.6914 - acc: 0.5815\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.6792 - acc: 0.5890\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 0s 97us/sample - loss: 0.6676 - acc: 0.5990\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 0s 123us/sample - loss: 0.6566 - acc: 0.6115\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 0s 107us/sample - loss: 0.6461 - acc: 0.6266\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.6352 - acc: 0.6617\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 0s 84us/sample - loss: 0.6245 - acc: 0.6767\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 0s 109us/sample - loss: 0.6138 - acc: 0.6967\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 0s 87us/sample - loss: 0.6024 - acc: 0.7043\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 0s 107us/sample - loss: 0.5914 - acc: 0.7293\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.5798 - acc: 0.7293\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 0s 122us/sample - loss: 0.5678 - acc: 0.7494\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.5559 - acc: 0.7644\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 0s 121us/sample - loss: 0.5441 - acc: 0.7719\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 0s 101us/sample - loss: 0.5321 - acc: 0.7895\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 0s 126us/sample - loss: 0.5200 - acc: 0.7945\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 0s 120us/sample - loss: 0.5086 - acc: 0.8045\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 0s 104us/sample - loss: 0.4971 - acc: 0.8120\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 0s 180us/sample - loss: 0.4854 - acc: 0.8246\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 0s 152us/sample - loss: 0.4741 - acc: 0.8446\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 0s 111us/sample - loss: 0.4621 - acc: 0.8571\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 0s 113us/sample - loss: 0.4498 - acc: 0.8672\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 0s 118us/sample - loss: 0.4376 - acc: 0.8722\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 0s 104us/sample - loss: 0.4253 - acc: 0.8747\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.4130 - acc: 0.8822\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 0s 102us/sample - loss: 0.4003 - acc: 0.8872\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.3874 - acc: 0.9073\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 0s 101us/sample - loss: 0.3744 - acc: 0.9173\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.3615 - acc: 0.9248\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 0s 105us/sample - loss: 0.3482 - acc: 0.9273\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 0s 121us/sample - loss: 0.3349 - acc: 0.9273\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 0s 175us/sample - loss: 0.3219 - acc: 0.9298\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.3093 - acc: 0.9373\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 0s 144us/sample - loss: 0.2968 - acc: 0.9373\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 0s 146us/sample - loss: 0.2849 - acc: 0.9348\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 0s 117us/sample - loss: 0.2738 - acc: 0.9373\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 0s 127us/sample - loss: 0.2628 - acc: 0.9398\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 0s 116us/sample - loss: 0.2520 - acc: 0.9424\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 0s 116us/sample - loss: 0.2419 - acc: 0.9449\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 0s 135us/sample - loss: 0.2318 - acc: 0.9499\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 0s 76us/sample - loss: 0.2220 - acc: 0.9549\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 0s 140us/sample - loss: 0.2130 - acc: 0.9574\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 0s 129us/sample - loss: 0.2040 - acc: 0.9549\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 0s 189us/sample - loss: 0.1956 - acc: 0.9574\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 0s 120us/sample - loss: 0.1880 - acc: 0.9624\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 0s 175us/sample - loss: 0.1805 - acc: 0.9649\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 0s 141us/sample - loss: 0.1731 - acc: 0.9724\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 0s 150us/sample - loss: 0.1665 - acc: 0.9724\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 0s 162us/sample - loss: 0.1601 - acc: 0.9749\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 0s 171us/sample - loss: 0.1540 - acc: 0.9749\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 0s 143us/sample - loss: 0.1478 - acc: 0.9774\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 0s 112us/sample - loss: 0.1424 - acc: 0.9774\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 0s 144us/sample - loss: 0.1369 - acc: 0.9774\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 0s 109us/sample - loss: 0.1314 - acc: 0.9774\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.1266 - acc: 0.9799\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 0s 71us/sample - loss: 0.1220 - acc: 0.9799\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.1173 - acc: 0.9799\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 0s 88us/sample - loss: 0.1130 - acc: 0.9825\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.1089 - acc: 0.9799\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.1049 - acc: 0.9850\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 0s 83us/sample - loss: 0.1013 - acc: 0.9850\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0977 - acc: 0.9850\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0942 - acc: 0.9875\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 0s 68us/sample - loss: 0.0907 - acc: 0.9875\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0873 - acc: 0.9875\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 0s 76us/sample - loss: 0.0840 - acc: 0.9900\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 0s 53us/sample - loss: 0.0810 - acc: 0.9900\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 0s 87us/sample - loss: 0.0779 - acc: 0.9875\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.0751 - acc: 0.9875\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0725 - acc: 0.9875\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 0s 76us/sample - loss: 0.0700 - acc: 0.9875\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 0s 77us/sample - loss: 0.0677 - acc: 0.9875\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 0s 91us/sample - loss: 0.0656 - acc: 0.9900\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 0s 100us/sample - loss: 0.0635 - acc: 0.9900\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 0s 110us/sample - loss: 0.0615 - acc: 0.9900\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 0s 88us/sample - loss: 0.0598 - acc: 0.9900\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 0s 74us/sample - loss: 0.0579 - acc: 0.9900\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 0s 74us/sample - loss: 0.0562 - acc: 0.9900\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 0s 75us/sample - loss: 0.0546 - acc: 0.9900\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 0s 73us/sample - loss: 0.0530 - acc: 0.9925\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 0s 61us/sample - loss: 0.0515 - acc: 0.9950\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 0s 71us/sample - loss: 0.0499 - acc: 0.9950\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 0s 77us/sample - loss: 0.0484 - acc: 0.9950\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 0s 80us/sample - loss: 0.0471 - acc: 0.9950\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 0s 60us/sample - loss: 0.0459 - acc: 0.9950\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0447 - acc: 0.9950\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 0s 76us/sample - loss: 0.0436 - acc: 0.9950\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0424 - acc: 0.9950\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 0s 74us/sample - loss: 0.0414 - acc: 0.9950\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 0s 96us/sample - loss: 0.0404 - acc: 0.9950\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 0s 68us/sample - loss: 0.0395 - acc: 0.9950\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 0s 45us/sample - loss: 0.0385 - acc: 0.9950\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 0s 94us/sample - loss: 0.0376 - acc: 0.9950\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 0s 72us/sample - loss: 0.0368 - acc: 0.9950\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 0s 52us/sample - loss: 0.0359 - acc: 0.9950\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 0s 82us/sample - loss: 0.0350 - acc: 0.9950\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 0s 90us/sample - loss: 0.0343 - acc: 0.9950\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 0s 67us/sample - loss: 0.0336 - acc: 0.9950\n"
     ]
    }
   ],
   "source": [
    "#Train the Model (Use pca data)\n",
    "fit_model3 = nn3.fit(pca_df, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34c65ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.032933469434597885, Accuracy: 0.9949875473976135\n"
     ]
    }
   ],
   "source": [
    "#scoring the model\n",
    "model_loss3, model_accuracy3 = nn3.evaluate(pca_df, y_train, verbose='auto')\n",
    "print(f\"Loss: {model_loss3}, Accuracy: {model_accuracy3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e34ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
